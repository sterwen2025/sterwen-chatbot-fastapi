================================================================================
                         CHATBOT RAG LOGIC FLOWCHART
                    (Verified against main.py - Nov 2024)
================================================================================

USER SENDS QUESTION
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│                         STEP 1: INPUT COLLECTION                              │
│                                                                               │
│   Frontend collects:                                                          │
│   • question: string (user's query)                                           │
│   • data_sources: ["Meeting Notes", "Factsheet", "Web Search"]               │
│   • start_date, end_date: optional filters (YYYY-MM-DD format)               │
│   • selected_funds: optional list of fund names                               │
│   • conversation_history: array of previous {question, answer} pairs          │
│   • model: "gemini-2.5-flash" | "gemini-2.5-pro" | "gemini-3-low-thinking"   │
│            | "gemini-3-high-thinking"                                         │
│                                                                               │
│   POST /api/chat/ask/stream → ask_gemini_with_rag_streaming()                │
└───────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│                    STEP 2: SMART RAG STRATEGY DECISION                        │
│                    (main.py lines 269-434)                                    │
│                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────┐    │
│   │  2.1 INTENT CLASSIFICATION - classify_intent()                      │    │
│   │      (Rule-based, no LLM call)                                      │    │
│   │                                                                      │    │
│   │  Transform patterns checked:                                         │    │
│   │  'summarize', 'summary', 'list', 'compare', 'contrast',             │    │
│   │  'tell me more', 'explain', 'elaborate', 'what do you mean',        │    │
│   │  'can you clarify', 'in other words', 'why', 'how',                 │    │
│   │  'what about the', 'regarding the', 'about the above'               │    │
│   │                                                                      │    │
│   │  IF pattern found AND conversation_history >= 2:                     │    │
│   │      → Return 'NO_RAG' (skip search, use existing context)          │    │
│   │  ELSE:                                                               │    │
│   │      → Return 'NEW_QUERY' (need search)                             │    │
│   └─────────────────────────────────────────────────────────────────────┘    │
│                              │                                                │
│                              ▼                                                │
│   ┌─────────────────────────────────────────────────────────────────────┐    │
│   │  2.2 SCOPE CHANGE DETECTION - detect_scope_change()                 │    │
│   │      (Only if Intent = NEW_QUERY)                                   │    │
│   │                                                                      │    │
│   │  FAST PATH (no LLM call):                                            │    │
│   │  Compare current vs previous filters:                                │    │
│   │    current = {start_date, end_date, sorted(selected_funds)}          │    │
│   │    previous = cached scope from rag_cache                            │    │
│   │                                                                      │    │
│   │  IF filters changed:                                                 │    │
│   │      → Return TRUE (scope changed, skip LLM - saves 200-800ms)      │    │
│   │                                                                      │    │
│   │  SLOW PATH (LLM call):                                               │    │
│   │  IF filters same, call Gemini to classify topic:                     │    │
│   │                                                                      │    │
│   │  Prompt: "Previous: {prev_question}"                                 │    │
│   │          "Current: {curr_question}"                                  │    │
│   │          "Are these SAME or DIFFERENT topics?"                       │    │
│   │                                                                      │    │
│   │  IF response = "DIFFERENT":                                          │    │
│   │      → Return TRUE (topic changed)                                   │    │
│   │  ELSE:                                                               │    │
│   │      → Return FALSE (same topic, can reuse cache)                   │    │
│   └─────────────────────────────────────────────────────────────────────┘    │
│                              │                                                │
│                              ▼                                                │
│   ┌─────────────────────────────────────────────────────────────────────┐    │
│   │  2.3 STRATEGY SELECTION - get_rag_strategy()                        │    │
│   │                                                                      │    │
│   │  Cache key: "{start_date}_{end_date}_{sorted(funds)}"               │    │
│   │                                                                      │    │
│   │  Decision Tree:                                                      │    │
│   │                                                                      │    │
│   │  Intent = NO_RAG?                                                    │    │
│   │      YES → Strategy = NO_RAG                                        │    │
│   │            (Skip all searches, ~1-2s response)                      │    │
│   │                                                                      │    │
│   │  Scope changed = TRUE?                                               │    │
│   │      YES → Strategy = RAG_NEW_QUERY                                 │    │
│   │            (Perform new hybrid search, ~4-7s)                        │    │
│   │                                                                      │    │
│   │  Scope changed = FALSE?                                              │    │
│   │      YES → Strategy = RAG_REUSE                                     │    │
│   │            (Use cached chunks, ~2-3s)                               │    │
│   └─────────────────────────────────────────────────────────────────────┘    │
└───────────────────────────────────────────────────────────────────────────────┘
        │
        ├────────────────────┬────────────────────┬────────────────────┐
        │                    │                    │                    │
        ▼                    ▼                    ▼                    │
┌───────────────┐    ┌───────────────┐    ┌───────────────┐           │
│   NO_RAG      │    │  RAG_REUSE    │    │ RAG_NEW_QUERY │           │
│               │    │               │    │               │           │
│ • Skip search │    │ • Check cache │    │ • Run hybrid  │           │
│ • Use conv    │    │ • Hit → use   │    │   search      │           │
│   history     │    │ • Miss → run  │    │ • Cache       │           │
│               │    │   new search  │    │   results     │           │
└───────┬───────┘    └───────┬───────┘    └───────┬───────┘           │
        │                    │                    │                    │
        └────────────────────┴────────────────────┴────────────────────┘
                                          │
                                          ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│                    STEP 3: HYBRID SEARCH                                      │
│                    (If Meeting Notes selected AND strategy != NO_RAG)          │
│                    (main.py lines 584-773)                                    │
│                                                                               │
│   ┌─────────────────────────────────────────────────────────────────────┐    │
│   │  Executed in PARALLEL using ThreadPoolExecutor(max_workers=2)       │    │
│   └─────────────────────────────────────────────────────────────────────┘    │
│                                                                               │
│   ┌─────────────────────────────┐    ┌─────────────────────────────┐         │
│   │    THREAD 1: VECTOR SEARCH  │    │    THREAD 2: BM25 SEARCH    │         │
│   │    perform_vector_search()  │    │    perform_bm25_search_on_  │         │
│   │                             │    │    meeting_chunks()         │         │
│   │                             │    │                             │         │
│   │  1. Generate query embedding│    │  1. Tokenize query          │         │
│   │     embedding_service.      │    │     - Remove stopwords      │         │
│   │     generate_embedding()    │    │     - Extract 3+ char words │         │
│   │     • Model: gemini-        │    │                             │         │
│   │       embedding-001         │    │  2. Build regex pattern     │         │
│   │     • Dimensions: 768       │    │     pattern = 'word1|word2' │         │
│   │     • Task: RETRIEVAL_QUERY │    │     Use BsonRegex for       │         │
│   │                             │    │     Cosmos DB compatibility │         │
│   │  2. Build Cosmos filter:    │    │                             │         │
│   │     IF date filter:         │    │  3. Fetch candidates        │         │
│   │       meeting_date: {       │    │     - Query chunks matching │         │
│   │         $gte: start_date,   │    │       any token             │         │
│   │         $lte: end_date      │    │     - Limit: top_k * 20     │         │
│   │       }                     │    │                             │         │
│   │     IF fund filter:         │    │  4. Fit BM25 model          │         │
│   │       fund_name: {          │    │     - k1 = 1.5 (TF sat)     │         │
│   │         $in: selected_funds │    │     - b = 0.75 (length norm)│         │
│   │       }                     │    │                             │         │
│   │                             │    │  5. Score all candidates    │         │
│   │  3. Execute $search:        │    │     BM25 = IDF * TF /       │         │
│   │     cosmosSearch {          │    │       (TF + k1*(1-b+b*L/A)) │         │
│   │       vector: embedding,    │    │                             │         │
│   │       path: "embedding",    │    │  6. Return top-k by score   │         │
│   │       k: 100,               │    │                             │         │
│   │       filter: cosmos_filter,│    │                             │         │
│   │       exact: true (ENN)     │    │                             │         │
│   │     }                       │    │                             │         │
│   │                             │    │                             │         │
│   │  4. Return ranked results   │    │                             │         │
│   │     with similarity scores  │    │                             │         │
│   └──────────────┬──────────────┘    └──────────────┬──────────────┘         │
│                  │                                  │                         │
│                  └──────────────┬───────────────────┘                         │
│                                 │                                             │
│                                 ▼                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐    │
│   │  RECIPROCAL RANK FUSION (RRF) - merge_vector_and_bm25_results()    │    │
│   │  (main.py lines 1182-1265)                                          │    │
│   │                                                                      │    │
│   │  Parameters:                                                         │    │
│   │    k = 60 (RRF constant from literature)                             │    │
│   │    vector_weight = 0.5 (50% weight for semantic search)              │    │
│   │    bm25_weight = 0.5 (50% weight for keyword matching)               │    │
│   │    top_k = 50 (final results to return)                              │    │
│   │                                                                      │    │
│   │  Algorithm:                                                          │    │
│   │  FOR each chunk in vector_results:                                   │    │
│   │      rrf_score += vector_weight * (1 / (k + rank + 1))              │    │
│   │                                                                      │    │
│   │  FOR each chunk in bm25_results:                                     │    │
│   │      rrf_score += bm25_weight * (1 / (k + rank + 1))                │    │
│   │                                                                      │    │
│   │  Benefits:                                                           │    │
│   │  • Chunks in BOTH lists get boosted                                  │    │
│   │  • No score normalization needed                                     │    │
│   │  • Handles different scoring scales naturally                        │    │
│   │                                                                      │    │
│   │  Output: Top 50 chunks sorted by hybrid_score                        │    │
│   └─────────────────────────────────────────────────────────────────────┘    │
│                                                                               │
│   Same hybrid search applied to:                                              │
│   • MeetingChunks collection (if "Meeting Notes" selected)                   │
│   • FactsheetChunks collection (if "Factsheet" selected)                     │
└───────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│                    STEP 4: FACTSHEET SEARCH                                   │
│                    (If "Factsheet" in data_sources)                           │
│                    (main.py lines 1268-1452)                                  │
│                                                                               │
│   Same hybrid search pattern:                                                 │
│                                                                               │
│   1. Vector Search on FactsheetChunks                                         │
│      • Collection: fund_reports.FactsheetChunks                               │
│      • Filter by: report_date range, UniqueName list                         │
│                                                                               │
│   2. BM25 Search on FactsheetChunks                                           │
│      • perform_bm25_search_on_chunks()                                        │
│                                                                               │
│   3. RRF Merge (50/50 weighting)                                              │
│                                                                               │
│   4. Deduplication by factsheet_id:                                           │
│      • Group chunks by factsheet_id                                           │
│      • Show matching sections with scores                                     │
│      • Include complete factsheet JSON once per unique factsheet              │
│                                                                               │
│   Output format:                                                              │
│   [Fund Name Factsheet (Date)] - filename.pdf                                 │
│   Matching Sections: performance (0.82), holdings (0.71)                      │
│   Complete Factsheet Data: {JSON}                                             │
└───────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│                    STEP 5: CONTEXT BUILDING                                   │
│                    (main.py lines 1806-1911)                                  │
│                                                                               │
│   context = "Available Data Sources:\n\n"                                     │
│                                                                               │
│   IF Meeting Notes chunks found:                                              │
│   ┌─────────────────────────────────────────────────────────────────────┐    │
│   │  === MEETING NOTES (RAG Retrieved) ===                              │    │
│   │                                                                      │    │
│   │  For each chunk (up to 100):                                         │    │
│   │    Date: {meeting_date}                                              │    │
│   │    Fund: {fund_name}                                                 │    │
│   │    Manager: {manager}                                                │    │
│   │    Chunk Type: {chunk_type}                                          │    │
│   │    Content: {text}                                                   │    │
│   │    Relevance Score: {score:.4f}                                      │    │
│   │    ---                                                               │    │
│   └─────────────────────────────────────────────────────────────────────┘    │
│                                                                               │
│   IF Factsheet chunks found:                                                  │
│   ┌─────────────────────────────────────────────────────────────────────┐    │
│   │  === RELEVANT FACTSHEET DATA (RAG Retrieved) ===                    │    │
│   │  Found {N} unique factsheet(s) with {M} relevant section(s)         │    │
│   │                                                                      │    │
│   │  [Fund Name Factsheet (Date)] - filename.pdf                         │    │
│   │  Matching Sections (N):                                              │    │
│   │    - section_type (relevance: 0.8234)                                │    │
│   │  Complete Factsheet Data: {JSON}                                     │    │
│   └─────────────────────────────────────────────────────────────────────┘    │
└───────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│                    STEP 6: PROMPT CONSTRUCTION                                │
│                    (main.py lines 1923-2007)                                  │
│                                                                               │
│   full_prompt = ""                                                            │
│                                                                               │
│   IF conversation_history exists:                                             │
│       full_prompt += "Previous conversation:\n\n"                             │
│       FOR item in conversation_history[-3:]:  # Last 3 only                   │
│           full_prompt += f"User: {question}\n"                                │
│           full_prompt += f"Assistant: {answer}\n\n"                           │
│       full_prompt += "---\n\n"                                                │
│                                                                               │
│   full_prompt += f"Question: {question}\n\n"                                  │
│                                                                               │
│   full_prompt += "Instructions:\n"                                            │
│   1. FORMATTING: Use ONLY markdown, NO HTML tags                              │
│   2. DATE AWARENESS: State dates of data referenced                           │
│   3. Read internal data (meeting notes + factsheets) FIRST                    │
│   4. Web search instructions (if enabled):                                    │
│      - Context-aware searches using entities from internal data               │
│      - Cite ALL web sources with clickable markdown links                     │
│      - Structure: Section 1 (Internal) + Section 2 (Web)                      │
│   5. Synthesize all information                                               │
│   6. Thorough explanations with context                                       │
│   7. Cite all sources with specifics                                          │
│   8. Use clear section headers                                                │
│   9. Give detailed, comprehensive responses                                   │
│   10. CITATION STYLE: Use "[Fund Name Factsheet (Date)]"                      │
│       NEVER use generic "Factsheet 1" or "Source: Factsheet 16"               │
│                                                                               │
│   full_prompt += f"Available data sources: {data_sources}\n\n"                │
│   full_prompt += f"Internal Data:\n{context}\n\n"                             │
│   full_prompt += "Now provide a comprehensive answer."                        │
└───────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│                    STEP 7: GEMINI API CALL                                    │
│                    (main.py lines 2017-2067)                                  │
│                                                                               │
│   Model Selection:                                                            │
│   ┌─────────────────────────────────────────────────────────────────────┐    │
│   │  Frontend model       │  Actual API model           │  Config       │    │
│   │  ─────────────────────┼─────────────────────────────┼─────────────  │    │
│   │  "gemini-2.5-flash"   │  gemini-2.5-flash           │  default      │    │
│   │  "gemini-2.5-pro"     │  gemini-2.5-pro             │  default      │    │
│   │  "gemini-3-low-think" │  gemini-3-pro-preview       │  think=low    │    │
│   │  "gemini-3-high-think"│  gemini-3-pro-preview       │  think=default│    │
│   └─────────────────────────────────────────────────────────────────────┘    │
│                                                                               │
│   IF "Web Search" in data_sources:                                            │
│       config = GenerateContentConfig(                                         │
│           tools=[Tool(google_search=GoogleSearch())],                         │
│           temperature=1,                                                      │
│           thinking_config=thinking_config  # if applicable                    │
│       )                                                                       │
│   ELSE:                                                                       │
│       config = GenerateContentConfig(                                         │
│           temperature=1,                                                      │
│           thinking_config=thinking_config  # if applicable                    │
│       )                                                                       │
│                                                                               │
│   response = gemini_client.models.generate_content_stream(                    │
│       model=actual_model,                                                     │
│       contents=full_prompt,                                                   │
│       config=config                                                           │
│   )                                                                           │
└───────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│                    STEP 8: STREAMING RESPONSE                                 │
│                    (main.py lines 2048-2099)                                  │
│                                                                               │
│   Backend yields Server-Sent Events (SSE):                                    │
│                                                                               │
│   yield ('STATUS', True)     →  data: {"searching": true}                     │
│   [perform hybrid searches]                                                   │
│   yield ('STATUS', False)    →  data: {"searching": false}                    │
│   yield ('GENERATING', True) →  data: {"generating": true}                    │
│                                                                               │
│   FOR each chunk in Gemini response:                                          │
│       IF chunk has thought (Gemini 3 thinking):                               │
│           yield ('THINKING', text)  →  data: {"thinking": "analyzing..."}    │
│       ELIF chunk has text:                                                    │
│           yield text                →  data: {"content": "Based on..."}      │
│                                                                               │
│   yield ('DONE', True)       →  data: {"done": true}                          │
│                                                                               │
│   Frontend handles:                                                           │
│   • "searching: true"  → Show "Finding relevant information..."               │
│   • "generating: true" → Show "Generating response..."                        │
│   • "thinking"         → Show thinking summary (Gemini 3 only)                │
│   • "content"          → Append to answer, render with ReactMarkdown          │
│   • "done"             → Save to conversation history                         │
└───────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
    RESPONSE DISPLAYED TO USER


================================================================================
                         STOP BUTTON FLOW
================================================================================

USER CLICKS STOP (ChatBot.tsx handleStop function)
        │
        ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│   1. abortControllerRef.current.abort()    // Cancel fetch request            │
│   2. abortControllerRef.current = null     // Clear reference                 │
│   3. setLoading(false)                     // Hide loading state              │
│   4. setSearchStatus('')                   // Clear status message            │
│   5. setThinkingSummary('')                // Clear thinking display          │
│                                                                               │
│   6. Update conversation:                                                     │
│      IF no answer yet:                                                        │
│          → Remove the question entirely from conversation                     │
│      ELSE (partial answer exists):                                            │
│          → Append "\n\n*[Response stopped by user]*" to answer               │
│                                                                               │
│   7. message.info('Request stopped')       // Show toast notification         │
│                                                                               │
│   Error handler catches AbortError and returns early (no error shown)         │
└───────────────────────────────────────────────────────────────────────────────┘


================================================================================
                         CACHE STRUCTURE
================================================================================

rag_cache = {
    "{start_date}_{end_date}_{sorted(funds)}": {
        "scope": {
            "question": "Original question text",
            "start_date": "2024-01-01",
            "end_date": "2024-12-31",
            "funds": ["Fund A", "Fund B"]  // sorted list
        },
        "chunks": [...],       // Cached RAG results (vector + BM25 merged)
        "timestamp": datetime  // When cached
    }
}

Example cache key: "2024-01-01_2024-12-31_['AQR', 'Citadel']"


================================================================================
                      PERFORMANCE SUMMARY
================================================================================

| Scenario                  | Time     | What happens                      |
|---------------------------|----------|-----------------------------------|
| NO_RAG (follow-up)        | 1-2s     | Skip search, use conversation     |
| RAG_REUSE (cache hit)     | 2-3s     | Use cached chunks                 |
| RAG_NEW_QUERY             | 4-7s     | Full hybrid search                |
| + Web Search              | +2-4s    | + Google Search Grounding         |

Breakdown for RAG_NEW_QUERY:
┌──────────────────────────┬───────────┐
│ Phase                    │ Time      │
├──────────────────────────┼───────────┤
│ Embedding generation     │ 0.3-0.5s  │
│ Vector search            │ 1-2s      │  ← Runs in parallel
│ BM25 search              │ 0.5-1s    │  ← Runs in parallel
│ RRF merge                │ 0.1s      │
│ Context building         │ 0.1s      │
│ Gemini API (first token) │ 1-3s      │
├──────────────────────────┼───────────┤
│ TOTAL                    │ 3-7s      │
└──────────────────────────┴───────────┘


================================================================================
                      DATABASE COLLECTIONS
================================================================================

1. Meetings.MeetingChunks (Meeting Notes)
   • chunk_id, meeting_id, chunk_index
   • chunk_type: "notes_part" | "conclusion" | "full_notes"
   • text: chunk content
   • embedding: 768-dimensional vector
   • fund_name, manager, meeting_date
   • Vector index: cosmosSearch on "embedding" field

2. fund_reports.FactsheetChunks (Factsheets)
   • chunk_id, factsheet_id
   • section_type: "performance" | "holdings" | "commentary"
   • text: chunk content
   • embedding: 768-dimensional vector
   • UniqueName, fund_name, report_date
   • full_document: complete factsheet JSON
   • Vector index: cosmosSearch on "embedding" field

3. Meetings.MeetingNotes (Fund names cache)
   • Preloaded at startup into ALL_FUND_NAMES global
   • Used for fund name matching in queries
